version: "3"
services:
  namenode:
    image: dhimasarbi/hadoop:3.3.4
    command: bash -c "/home/hadoop/hadoop-cmd.sh start namenode"
    container_name: namenode
    environment:
      - HADOOP_HOSTNAME=namenode
    ports:
      - target: 9870
        published: 9870
        protocol: tcp
        mode: host
      - target: 9000
        published: 9000
        protocol: tcp
        mode: host
      - target: 9868
        published: 9868
        protocol: tcp
        mode: host
      - target: 50070
        published: 50070
        protocol: tcp
        mode: host
      - target: 8088
        published: 8088
        protocol: tcp
        mode: host
    volumes:
      - hdfs-master-data:/home/hadoop/data/nameNode
      - /hadoop/namenode_data:/data
      - /hadoop/user_data:/home/user
    networks:
      - network
    deploy:
      mode: global
      endpoint_mode: dnsrr
      placement:
        constraints: [node.hostname == namenode]

  datanode:
    image: dhimasarbi/hadoop:3.3.4
    command: bash -c "/home/hadoop/hadoop-cmd.sh start"
    depends_on:
      - "namenode"
    ports:
      - target: 9867
        published: 9867
        protocol: tcp
        mode: host
      - target: 9866
        published: 9866
        protocol: tcp
        mode: host
      - target: 9865
        published: 9865
        protocol: tcp
        mode: host
      - target: 9864
        published: 9864
        protocol: tcp
        mode: host
    volumes:
      - hdfs-worker-data:/home/hadoop/data/dataNode
      - /hadoop/datanode_data:/data
    networks:
      - network
    deploy:
      replicas: 2
      placement:
        constraints: [node.hostname != namenode]

volumes:
  hdfs-master-data:
    name: "hdsf_master_data_swarm"
  hdfs-master-checkpoint-data:
    name: "hdsf_master_checkpoint_data_swarm"
  hdfs-worker-data:
    name: "hdsf_worker_data_swarm"

networks:
  network:
    driver: overlay
    attachable: true
